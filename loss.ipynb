{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "loss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMdww3XvITFu0x/MhizILhp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syx1990/deep-learning/blob/main/loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIdwWygPm_nW",
        "outputId": "ee6961bc-a6a7-4151-d204-611c672f4384"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# !/usr/bin/python3\n",
        "# author by : yuxiangShi\n",
        "# tell  by: 18538187569\n",
        "# desc by: 反向传播：损失函数 loss = (w + 1)2 然后求导\n",
        "\n",
        "# 导入包\n",
        "import tensorflow as tf\n",
        "\n",
        "# 版本号\n",
        "print(tf.__version__)\n",
        "\n",
        "# 定义权重和学习率\n",
        "w = tf.Variable(tf.constant(5,dtype=tf.float32)) # 需要训练的\n",
        "lr = 0.2\n",
        "\n",
        "# 定义循环数\n",
        "epoch = 40 # 四十次\n",
        "\n",
        "for epoch in range(epoch): # for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。\n",
        "  with tf.GradientTape() as tape: # with结构到grads框起了梯度的计算过程。\n",
        "      loss = tf.square(w + 1)\n",
        "  grades = tape.gradient(loss,w) # .gradient函数告知谁对谁求导\n",
        "\n",
        "  w.assign_sub(lr*grades) # .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads\n",
        "  print(\"After %s epoch,w is %f,loss is %f\" % (epoch, w.numpy(), loss))\n",
        "\n",
        "# lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程\n",
        "# 最终目的：找到 loss 最小 即 w = -1 的最优参数w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n",
            "After 0 epoch,w is 2.600000,loss is 36.000000\n",
            "After 1 epoch,w is 1.160000,loss is 12.959999\n",
            "After 2 epoch,w is 0.296000,loss is 4.665599\n",
            "After 3 epoch,w is -0.222400,loss is 1.679616\n",
            "After 4 epoch,w is -0.533440,loss is 0.604662\n",
            "After 5 epoch,w is -0.720064,loss is 0.217678\n",
            "After 6 epoch,w is -0.832038,loss is 0.078364\n",
            "After 7 epoch,w is -0.899223,loss is 0.028211\n",
            "After 8 epoch,w is -0.939534,loss is 0.010156\n",
            "After 9 epoch,w is -0.963720,loss is 0.003656\n",
            "After 10 epoch,w is -0.978232,loss is 0.001316\n",
            "After 11 epoch,w is -0.986939,loss is 0.000474\n",
            "After 12 epoch,w is -0.992164,loss is 0.000171\n",
            "After 13 epoch,w is -0.995298,loss is 0.000061\n",
            "After 14 epoch,w is -0.997179,loss is 0.000022\n",
            "After 15 epoch,w is -0.998307,loss is 0.000008\n",
            "After 16 epoch,w is -0.998984,loss is 0.000003\n",
            "After 17 epoch,w is -0.999391,loss is 0.000001\n",
            "After 18 epoch,w is -0.999634,loss is 0.000000\n",
            "After 19 epoch,w is -0.999781,loss is 0.000000\n",
            "After 20 epoch,w is -0.999868,loss is 0.000000\n",
            "After 21 epoch,w is -0.999921,loss is 0.000000\n",
            "After 22 epoch,w is -0.999953,loss is 0.000000\n",
            "After 23 epoch,w is -0.999972,loss is 0.000000\n",
            "After 24 epoch,w is -0.999983,loss is 0.000000\n",
            "After 25 epoch,w is -0.999990,loss is 0.000000\n",
            "After 26 epoch,w is -0.999994,loss is 0.000000\n",
            "After 27 epoch,w is -0.999996,loss is 0.000000\n",
            "After 28 epoch,w is -0.999998,loss is 0.000000\n",
            "After 29 epoch,w is -0.999999,loss is 0.000000\n",
            "After 30 epoch,w is -0.999999,loss is 0.000000\n",
            "After 31 epoch,w is -1.000000,loss is 0.000000\n",
            "After 32 epoch,w is -1.000000,loss is 0.000000\n",
            "After 33 epoch,w is -1.000000,loss is 0.000000\n",
            "After 34 epoch,w is -1.000000,loss is 0.000000\n",
            "After 35 epoch,w is -1.000000,loss is 0.000000\n",
            "After 36 epoch,w is -1.000000,loss is 0.000000\n",
            "After 37 epoch,w is -1.000000,loss is 0.000000\n",
            "After 38 epoch,w is -1.000000,loss is 0.000000\n",
            "After 39 epoch,w is -1.000000,loss is 0.000000\n"
          ]
        }
      ]
    }
  ]
}